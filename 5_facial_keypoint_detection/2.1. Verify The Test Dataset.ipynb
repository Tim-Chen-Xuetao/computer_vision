{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Architecture\n",
    "\n",
    "Recall that CNN's are defined by a few types of layers:\n",
    "* Convolutional layers\n",
    "* Maxpooling layers\n",
    "* Fully-connected layers\n",
    "\n",
    "You are required to use the above layers and encouraged to add multiple convolutional layers and things like dropout layers that may prevent overfitting. You are also encouraged to look at literature on keypoint detection, such as [this paper](https://arxiv.org/pdf/1710.00977.pdf), to help you determine the structure of your network.\n",
    "\n",
    "\n",
    "### TODO: Define your model in the provided file `models.py` file\n",
    "\n",
    "This file is mostly empty but contains the expected name and some TODO's for creating your model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv5): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=25600, out_features=5120, bias=True)\n",
       "  (fc2): Linear(in_features=5120, out_features=1024, bias=True)\n",
       "  (fc3): Linear(in_features=1024, out_features=136, bias=True)\n",
       "  (drop): Dropout(p=0.5)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            import torch\n",
    "from models import Net\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net = Net()\n",
    "#net.to(device)\n",
    "\n",
    "## TODO: load the best saved model parameters (by your path name)\n",
    "## You'll need to un-comment the line below and add the correct name for *your* saved model\n",
    "net.load_state_dict(torch.load('saved_models/keypoints_model_1.pt'))\n",
    "\n",
    "## print out your net and prepare it for testing (uncomment the line below)\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "# the dataset we created in Notebook 1 is copied in the helper file `data_load.py`\n",
    "from data_load import FacialKeypointsDataset\n",
    "# the transforms we defined in Notebook 1 are in the helper file `data_load.py`\n",
    "from data_load import Rescale, RandomCrop, Normalize, ToTensor\n",
    "\n",
    "## TODO: define the data_transform using transforms.Compose([all tx's, . , .])\n",
    "# order matters! i.e. rescaling should come before a smaller crop\n",
    "data_transform = transforms.Compose([Rescale(250),\n",
    "                                     RandomCrop(224),\n",
    "                                     Normalize(),\n",
    "                                     ToTensor()])\n",
    "# testing that you've defined a transform\n",
    "assert(data_transform is not None), 'Define a data_transform'\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the test data, using the dataset class\n",
    "# AND apply the data_transform you defined above\n",
    "batch_size = 50\n",
    "# create the test dataset\n",
    "test_dataset = FacialKeypointsDataset(csv_file='/data/test_frames_keypoints.csv',\n",
    "                                             root_dir='/data/test/',\n",
    "                                             transform=data_transform)\n",
    "\n",
    "# load test data in batches\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False, \n",
    "                          num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def net_sample_output():\n",
    "    \n",
    "    totalloss = 0\n",
    "    totalnum = 0\n",
    "        \n",
    "    # iterate through the test dataset\n",
    "    for i, sample in enumerate(test_loader):\n",
    "        \n",
    "        # get sample data: images and ground truth keypoints\n",
    "        images = sample['image']\n",
    "        key_pts = sample['keypoints']\n",
    "        \n",
    "        key_pts = key_pts.type(torch.FloatTensor)\n",
    "        images = images.type(torch.FloatTensor)\n",
    "        \n",
    "        #key_pts = key_pts.view(key_pts.size(0), -1)\n",
    "        #images.to(device) \n",
    "        #key_pts.to(device)\n",
    "        \n",
    "        # convert images to FloatTensors\n",
    "        #images = images.type(torch.FloatTensor)\n",
    "\n",
    "        # forward pass to get net output\n",
    "        output_pts = net(images)\n",
    "        \n",
    "        # reshape to batch_size x 68 x 2 pts\n",
    "        output_pts = output_pts.view(output_pts.size()[0], 68, -1)\n",
    "        \n",
    "        loss = criterion(output_pts, key_pts)\n",
    "        \n",
    "        print('Avg. Loss: {}'.format(loss.item()/10))\n",
    "        \n",
    "        # calculate the loss between predicted and target keypoints\n",
    "        totalloss = totalloss + loss\n",
    "        totalnum =  totalnum + 1\n",
    "        \n",
    "        if totalnum == 5: \n",
    "            break\n",
    "               \n",
    "        del images\n",
    "        del key_pts\n",
    "                        \n",
    "    print('Total Avg. Loss: {}'.format(totalloss/totalnum/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xfei/Desktop/course/data_load.py:40: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  key_pts = self.key_pts_frame.iloc[idx, 1:].as_matrix()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.0052962925285100935\n",
      "Avg. Loss: 0.004224289208650589\n",
      "Avg. Loss: 0.004312120750546456\n",
      "Avg. Loss: 0.020882415771484374\n",
      "Avg. Loss: 0.004787022992968559\n",
      "Total Avg. Loss: 0.007900427095592022\n"
     ]
    }
   ],
   "source": [
    "net_sample_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
